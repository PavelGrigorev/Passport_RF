# -*- coding: utf-8 -*-
"""pass.ipynb

Automatically generated by Colaboratory.

"""

#PASS API
#created by Sergey Sychov for Neural Univercity 2021.01.24
import time
import cv2
import torch
import torch.backends.cudnn as cudnn
from numpy import random
import os, sys

from models.classes import * #Важно! рассположить всю папку в той же директории что и сам модуль  

from OCR_model import OCR_predict

#---- IMPORTANT GLOBALS------------------------------------------------------------------------------------------------------------------------
WEIGHTS_PATH = 'weights/merger_yolo.pt' #Change to real path at runtime
TEST_IMAGE = 'test/pass1.jpg' #Only for test. Change to real path for test image or image's folder
SAVE_PATH = 'test/pass_res.jpeg' #Change to real path for svaing results
IMG_SZ = 640
#-------------------------------------------------------------------------------------------------------------------

DEBUG = False #for debbugin set True

CLASSES = [ 'фото', 'None', 'дата выдачи', '№ подр', 'выдан 1 строка', 'выдан 2 строка', 'выдан 3 строка',
            'место рожд. 1 строка', 'место рожд. 2 строка', 'место рожд. 3 строка',
             'серия', 'подпись', 'фамилия', 'имя', 'отчетсво', 'пол', 'дата рожд']

#CLASSES = [str(i + 1) for i in range(19)]


def post_processing(prediction):
  result_pred = [] # готовый вывод
  vidan, mesto = '', '' # Для конкатенации нескольких классов
  IDX_vidan, IDX_mesto = (4, 5, 6), (7, 8, 9)
  for idx, text in prediction:
    if idx in IDX_vidan:
      vidan += ' ' + text
    elif idx in IDX_mesto:
      mesto += ' ' + text
    else:
      result_pred.append(text)
  
  result_pred.extend([vidan.lstrip(), mesto.lstrip()])
  
  return result_pred



def run(input_img_path=TEST_IMAGE, save_ph=SAVE_PATH, weights_path=WEIGHTS_PATH, size_img=416, classes_names=CLASSES):
  '''
  run: Определяет объекты на фотографиях паспортов РФ 
  Классифицирует на 12 классов: фото владельца - 0, подразделение выдавшее паспорт 1, дата выдачи -2, номер подразделения 3,
  серия номер пасспорта 4, подпись 5, фамилия 6, имя 7, отчетсво 8, пол 9, дата рождения -10, место рождения 11
  входы:
    - input_img_path - путь к имени файла фото паспорта. Либо дирректорию с несолькими фото
    - save_ph - путь к файлу результата 
    - weights_path- путь к весам предобученоой модели YOLOv5 - Важно! указатб реальный путь к весам модели
    - размер (ширина) выходящих изображений, сеть обучалоась на этих данных 416
  выходы:
    - путь к файлу результата (имя совпадает с именем входящего файла ), либо последнему файлу результатов,если на вход было подана дирректория
  '''
  t0 = time.time()
  source = input_img_path #'images'
  weights = weights_path
  view_img = False #True show img
  save_txt = False #True save res to txt
  imgsz = size_img

  bbox_list, label_list = [], []
  #save_dir = save_dir

  set_logging()
  device = select_device()
  half = device.type != 'cpu'  # half precision only supported on CUDA


  set_logging()
  device = select_device()
  half = device.type != 'cpu'  # half precision only supported on CUDA

  # Load model
  model = attempt_load(weights, map_location=device)  # load FP32 model
  imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size
  if half:
    model.half()  # to FP16

  vid_path, vid_writer = None, None
  save_img = True
  dataset = LoadImages(source, img_size=imgsz)

  # Get names and colors
  names = model.module.names if hasattr(model, 'module') else model.names
  if classes_names:
    names = classes_names

  colors = [[random.randint(100, 150) for _ in range(3)] for _ in names]

  # Run inference
  
  img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img
  _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once
  for path, img, im0s, vid_cap in dataset:
      img = torch.from_numpy(img).to(device)
      img = img.half() if half else img.float()  # uint8 to fp16/32
      img /= 255.0  # 0 - 255 to 0.0 - 1.0
      if img.ndimension() == 3:
          img = img.unsqueeze(0)

      # Inference
      t1 = time_synchronized()
      #opt.augment
      arg = False
      pred = model(img, augment=arg)[0]

      # Apply NMS
      conf_thres = 0.55 #opt.conf_thres
      iou_thres = 0.45 #opt.iou_thres

      pred = non_max_suppression(pred, conf_thres, iou_thres)
      t2 = time_synchronized()

      # Process detections
      for i, det in enumerate(pred):  # detections per image
          p, s, im0 = Path(path), '', im0s
          save_path = save_ph
          
          #txt_path = str(save_dir+'/labels/'+ p.stem) + ('_%g' % dataset.frame if dataset.mode == 'video' else '')

          s += '%gx%g ' % img.shape[2:]  # print string
          gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh
          if len(det):
              # Rescale boxes from img_size to im0 size
              det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()

                # Print results
              
              for c in det[:, -1].unique():
                  n = (det[:, -1] == c).sum()  # detections per class
                  s += '%g %ss, ' % (n, names[int(c)])  # add to string
              

              # Write results
              save_conf = False
              for *xyxy, conf, cls in reversed(det):
                  if save_txt:  # Write to file
                      xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh
                      line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format
                      with open(txt_path + '.txt', 'a') as f:
                          f.write(('%g ' * len(line)).rstrip() % line + '\n')
                  
                  if save_img or view_img:  # Add bbox to image
                      #label = '%s %.2f' % (names[int(cls)], conf)
                      label = '%s' % (names[int(cls)])
                      label_list.append(int(cls))
                      bbox_list.append(list(map(int, xyxy)))
                      #plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)
                      plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)
                  

            # Print time (inference + NMS)
          if DEBUG: print('%sDone. (%.3fs)' % (s, t2 - t1))

            # Stream results
          if view_img:
              cv2.imshow(str(p), im0)
              if cv2.waitKey(1) == ord('q'):  # q to quit
                  raise StopIteration

            # Save results (image with detections)
          if save_img:
                if dataset.mode == 'images':
                    cv2.imwrite(save_path, im0)
                else:
                    if vid_path != save_path:  # new video
                        vid_path = save_path
                        if isinstance(vid_writer, cv2.VideoWriter):
                            vid_writer.release()  # release previous video writer

                        fourcc = 'mp4v'  # output video codec
                        fps = vid_cap.get(cv2.CAP_PROP_FPS)
                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))
                    vid_writer.write(im0)

  #if save_txt or save_img:
  #    s = f"\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}" if save_txt else ''
  #    if DEBUG: print(f"Results saved to {save_dir}{s}")

  return sorted(list(zip(label_list, bbox_list)))

  if DEBUG: 
      print('Done. (%.3fs)' % (time.time() - t0))
  return save_path


## Main run
boxes = run(input_img_path=TEST_IMAGE, save_ph=SAVE_PATH, weights_path=WEIGHTS_PATH, size_img=640, classes_names=CLASSES)
predict, idx_list = OCR_predict(TEST_IMAGE, boxes)

result = post_processing(zip(idx_list, predict))

print(*result, sep='\n')
#print(*predict, sep='\n')
# with open(os.path.join('test/result', 'pass_res.txt'), 'w') as f:
#       f.write('\n'.join(predict))

# for img_path in os.listdir('test')[1:-1]:
#   img_name, img_ext = img_path.split('.')
#   if img_ext in ['JPG', 'jpg', 'jpeg', 'png']:
#     test_image = os.path.join('test', img_path)
#     save_path = os.path.join('test/result', img_path)
#     boxes = run(input_img_path=test_image, save_ph=save_path, weights_path=WEIGHTS_PATH, size_img=640, classes_names=CLASSES)
    
#     print(*predict, sep='\n')
    

